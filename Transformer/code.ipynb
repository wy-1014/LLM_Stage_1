{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be28b222",
   "metadata": {},
   "source": [
    "\n",
    "<b><font face=\"微软雅黑\" size=\"5\" color=\"lightblue\">Transformer网络详解</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2afd0ce",
   "metadata": {},
   "source": [
    "<b><font face=\"微软雅黑\" size=\"4\" color=\"lightblue\">导入相关库</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5abfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c1285",
   "metadata": {},
   "source": [
    "<b><font face=\"微软雅黑\" size=\"4\" color=\"lightblue\">Transformer - Encoder</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8367541e",
   "metadata": {},
   "source": [
    "Encoder输入：文本的token通过Word Embedding映射为固定维度的词向量，一般为512维，再将词向量序列和位置编码相加变为多头注意力网络的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0df9a",
   "metadata": {},
   "source": [
    "PositionalEncoding:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90773b9",
   "metadata": {},
   "source": [
    "<img src=\"Encoder.png\" width=\"480\" height=\"400\" alt=\"Encoder\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c248dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义参数\n",
    "embed_size = 512  #词嵌入维度, 每个token经过Embeding后的维度\n",
    "head_nums = 8     #多头注意力头数\n",
    "head_size = embed_size // head_nums #每个头的输入维度\n",
    "hidden_size = 512 #每个头的输出维度\n",
    "\n",
    "#单个注意力代码实现\n",
    "#*args：接收任意个数的位置参数，打包成元组 (tuple) 传入函数内部；\n",
    "#**kwargs：接收任意个数的关键字参数，打包成字典 (dict) 传入函数内部；\n",
    "class SingleAttention(nn.Module):\n",
    "    #初始化\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        #初始化父类\n",
    "        super.__init__(*args,**kwargs)\n",
    "        #定义Q,K,V的线性变换层\n",
    "        self.Q_linear = nn.Linear(head_size,hidden_size)\n",
    "        self.K_linear = nn.Linear(head_size,hidden_size)\n",
    "        self.V_linear = nn.Linear(head_size,hidden_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size, seq_len, head_size = x.size()\n",
    "        #获取Q,K,V\n",
    "        Q = self.Q_linear(x)   #查询矩阵维度：(batch_size, seq_len, hidden_size)\n",
    "        K = self.K_linear(x)   #键矩阵维度：(batch_size, seq_len, hidden_size) \n",
    "        V = self.V_linear(x)   #值矩阵维度：(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        #计算注意力分数\n",
    "        #注意力分数维度：(batch_size, seq_len, seq_len) 每个位置与其他位置的相关性\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(head_size)  \n",
    "\n",
    "        #计算注意力权重, 在最后一维上进行softmax归一化，得到每个位置对当前token的权重，且所有权重和为1\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        #计算加权和\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c5e590",
   "metadata": {},
   "source": [
    "单头注意力计算：输入的数据维度为 batch_size, head_nums, seq_length, 词向量维度/head_nums;输入数据分别乘wq，wk，wv 得到Q,K,V，Q,K矩阵点乘输出不同位置词向量和其余位置词向量之间的余弦相似度，再通过Scaling（结果除根号dk,dk是单个头输入的词向量维度）操作防止乘积过大而导致softmax归一化后梯度极小的问题，最后将softmaxs输出的结果和V矩阵相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512\n",
    "head_nums = 12\n",
    "head_size = embed_size // head_nums\n",
    "hidden_size = 512\n",
    "dropout = 0.1\n",
    "\n",
    "# 单个注意力\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # 计算Q，K，V\n",
    "        self.query = nn.Linear(head_size, hidden_size)\n",
    "        self.key = nn.Linear(head_size, hidden_size)\n",
    "        self.value = nn.Linear(head_size, hidden_size)\n",
    "\n",
    "        # 创建mask\n",
    "        self.register_buffer(\"attention_mask\", torch.tril(torch.ones(head_size, head_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, head_size = x.size()\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        weight = q @ k.transpose(-2,-1)\n",
    "        weight = weight.masked_fill(self.attention_mask[:seq_len,:seq_len] == 0, float('-inf')) / math.sqrt(head_size)\n",
    "        weight = F.softmax(weight, -1)\n",
    "        weight = self.dropout(weight)\n",
    "        output = weight @ v\n",
    "        return  output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15b924",
   "metadata": {},
   "source": [
    "多头注意力计算：多头注意力即多个单头的叠加，针对一个词向量来说，假如一个词向量的维度是512，头数为8，那么每个头的输入就是64维，每个头会将词向量的不同部分作为输入计算，关注词向量不同维度下的特征，最后将所有头的输出拼接。多头注意力输出后会经过残差和层归一化操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2352d43",
   "metadata": {},
   "source": [
    "<b><font face=\"微软雅黑\" size=\"4\" color=\"lightblue\">Transformer - Decoder</font></b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
